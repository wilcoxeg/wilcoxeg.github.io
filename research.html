<!DOCTYPE html>
<html>
<head>
  <link rel="stylesheet" href="stylesheet.css">
</head>

<div id="navbar">
<h4> <a href="index.html"> Ethan Gotlieb Wilcox </a> • <a href="research.html"> Research</a> • <a href="teaching.html"> Teaching</a> • <a href="errata.html"> Miscellany </a> <br> <hr> </h4> 
</div>

<b> Computational Syntax </b> <br> <br>

To what extent are syntactic biases innate in the human language faculty? And how are they deployed during language processing and language learning? This research program seeks to use recent developments in language modeling to provide new empirical insight into these questions. By assessing how model architecture and training data affect the learning outcomes for contemporary neural network architectures, we can uncover the computational properties that guide language use in humans. <br><br>

<a href="https://arxiv.org/pdf/2010.05725.pdf" target="_blank">[•]</a> Ethan Wilcox, Peng Qian, Richard Futrell, Ryosuke Kohita, Roger P. Levy, and Miguel Ballesteros. 2020. Structural Supervision Improves Few-Shot Learning and Syntactic Generalization in Neural Language Models. <i> EMNLP </i> 2020 <br>
<a href="https://arxiv.org/pdf/2005.03692.pdf" target="_blank">[•]</a> Jennifer Hu, Jon Gauthier, Ethan Wilcox, Peng Qian, and Roger Levy. A systematic assessment of syntactic generalization in neural language models. <i> ACL </i> 2020 <br>
<a href="https://arxiv.org/pdf/1809.00042.pdf" target="_blank">[•]</a> Wilcox, Ethan; Levy, Roger; Morita, Takashi; Futrell, Richard. 2018. What do RNN Language Models Learn about the Filler-Gap Dependency? <i> Proceedings of Blackbox NLP at EMNLP 2018 </i> <br>
<br>

<p> A talk I gave recently at the Conference on Empirical Methods in Natural Language Processing (EMNLP) </p>
<div style="text-align: center">
<iframe height="350" width="600" src="https://www.youtube-nocookie.com/embed/AOnaGaQLA_U"></iframe> <br>
<br>
</div>

<br>

<b> Experimental Semantics: Presupposition and Exhaustivity </b> <br> <br>

The interpretation of linguistics utterances depends on both the logical form of the utterance (its semantics), as well as the context in which it was produced. In this work I use methods from experimental psycholinguistics as well as Bayesian modeling to develop and evaluate theories of presupposition, presupposition accommodation, and exhaustivity. Specifically, I ask how listeners cooperatively adjust their representations of common ground to accommodate assumptions that have been made by speakers during discourse. <br><br>
<u>Representative Publications</u><br>
<a href="https://mindmodeling.org/cogsci2019/papers/0519/0519.pdf" target="_blank"> [•] </a> Ethan Wilcox​ and Benjamin Spector. 2019. ​The Role of Prior Beliefs in The Rational Speech Act Model of Pragmatics: Exhaustivity as a Case Study​. <i> Proceedings of CogSci 2019 </i> <br> <br>

<b> Neural Networks as Models of Human Language Processing </b> <br> <br>

Neural networks are everywhere, and have achieved state of the art performance on many NLP tasks. But to what extent do they serve as good models for human linguistic processing? This work seeks to benchmark neural network models by treating them like subjects in a psycholinguistic experiment. By deriving word-by-word predictions, and comparing these to classic studies of human word-by-word reading times, we can uncover the ways in which models' learned representations are driving behavior that is similar to (and different from) human behavior. <br><br>

<a href="https://arxiv.org/pdf/2006.01912.pdf" target="_blank">[•]</a> Ethan Wilcox, Jon Gauthier, Jennifer Hu, Peng Qian and Roger Levy. 2020. On the Predictive Power of Neural Language Models for Human Real-Time Comprehension Behavior. <i> CogSci 2020 </i> <br>
<a href="https://www.aclweb.org/anthology/N19-1004" target="_blank">[•]</a> Richard Futrell, Ethan Wilcox, Takashi Morita, Miguel Ballesteros, Roger Levy. 2019. Neural Language Models as Psycholinguistic Subjects: Representation of Syntactic State. <i> NAACL-HLT 2019 </i> <br>

